{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.8 64-bit",
   "display_name": "Python 3.6.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "63c14ee7ec2db36c69c3e65307bf4a6c339f1042a6536b772b612cd09d46f60d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /etc/issue.net # OS\n",
    "!cat /proc/cpuinfo  # CPU\n",
    "!cat /proc/meminfo  # RAM\n",
    "!df -h              # Disk\n",
    "!nvidia-smi         # GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!cd \"gdrive/My Drive/Colab Notebooks\"; ls;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"gdrive/My Drive/Colab Notebooks/DQN-hw/requirements.txt\" .\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('content', 'gdrive', 'My Drive', 'Colab Notebooks', 'DQN-hw'))\n",
    "\n",
    "!cp -r \"gdrive/My Drive/Colab Notebooks/DQN-hw/PyGame-Learning-Environment\" .\n",
    "!cd \"PyGame-Learning-Environment\"; pip install -e .;\n",
    "sys.path.append('PyGame-Learning-Environment')\n",
    "!cp -r \"gdrive/My Drive/Colab Notebooks/DQN-hw/utils\" .\n",
    "from utils.env import Environment\n",
    "!cp -r \"gdrive/My Drive/Colab Notebooks/DQN-hw/agent\" .\n",
    "from agent.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def convert_to_tensor(state, action, next_state, reward, done):\n",
    "    state = torch.from_numpy(state).float() / 255.0\n",
    "\n",
    "    action_onehot = np.zeros(2)\n",
    "    action_onehot[action] = 1\n",
    "    action_onehot = np.expand_dims(action_onehot, axis=0)\n",
    "    action = torch.from_numpy(action_onehot).float()\n",
    "\n",
    "    next_state = torch.from_numpy(next_state).float() / 255.0\n",
    "    reward = torch.tensor([[reward]]).float()\n",
    "    done = torch.tensor([[done]])\n",
    "\n",
    "    return state, action, next_state, reward, done\n",
    "\n",
    "\n",
    "def train(hParam, env, agent):\n",
    "    num_episodes = int(1e6)\n",
    "    best = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "\n",
    "        while not env.game_over():\n",
    "            action = agent.getAction(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # PyGameDisplay to OpenCV\n",
    "            frame = env.get_screen()\n",
    "            frame = np.rot90(frame, k=1)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            frame = frame[::-1]\n",
    "\n",
    "            output.clear()\n",
    "            cv2_imshow(frame)\n",
    "            time.sleep(0.1)\n",
    "            # output.clear()\n",
    "\n",
    "            state_, action_, next_state_, reward_, done_ = convert_to_tensor(state, action, next_state, reward, done)\n",
    "\n",
    "            agent.memory.push(state_, action_, next_state_, reward_, done_)\n",
    "            loss = agent.updateQnet()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        if episode > 100:\n",
    "            if episode % hParam[\"TARGET_UPDATE\"] == 0:\n",
    "                agent.updateTargetNet()\n",
    "\n",
    "            if episode % 10 == 1:\n",
    "                print('Episode: {}, Reward: {:.3f}, Loss: {:.3f}'.format(episode, env.total_reward, loss))\n",
    "                if env.total_reward > best:\n",
    "                    agent.save()\n",
    "                    best = env.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "hParam = {\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"TARGET_UPDATE\": 5\n",
    "}\n",
    "env = Environment(device, display=True)\n",
    "chulsoo = Agent(env.action_set, hParam)\n",
    "train(hParam, env, chulsoo)"
   ]
  }
 ]
}