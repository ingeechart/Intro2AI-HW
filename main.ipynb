{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /etc/issue.net # OS\n",
    "!cat /proc/cpuinfo  # CPU\n",
    "!cat /proc/meminfo  # RAM\n",
    "!df -h              # Disk\n",
    "!nvidia-smi         # GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!cd \"gdrive/My Drive/Colab Notebooks\"; ls;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"gdrive/My Drive/Colab Notebooks/Intro2AI-HW/requirements.txt\" .\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('content', 'gdrive', 'My Drive', 'Colab Notebooks', 'Intro2AI-HW'))\n",
    "\n",
    "!cp -r \"gdrive/My Drive/Colab Notebooks/Intro2AI-HW/PyGame-Learning-Environment\" .\n",
    "!cd \"PyGame-Learning-Environment\"; pip install -e .;\n",
    "sys.path.append('PyGame-Learning-Environment')\n",
    "!cp -r \"gdrive/My Drive/Colab Notebooks/Intro2AI-HW/utils\" .\n",
    "from utils.env import Environment\n",
    "!cp -r \"gdrive/My Drive/Colab Notebooks/Intro2AI-HW/agent\" .\n",
    "from agent.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "if torch.__version__ <= '1.1.0':\n",
    "    from tensorboardX import SummaryWriter\n",
    "else:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "\n",
    "def train(hParam, env, agent):\n",
    "    best = 0\n",
    "    global_steps = 0\n",
    "    i_episode = 0\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    print('TRAIN STARTS')\n",
    "\n",
    "    while(hParam['MAX_ITER'] > global_steps ):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        i_episode += 1\n",
    "\n",
    "        while not env.game_over():\n",
    "            global_steps += 1\n",
    "\n",
    "            action = agent.getAction(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # PyGameDisplay to OpenCV\n",
    "            frame = env.get_screen()\n",
    "            frame = np.rot90(frame, k=1)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            frame = frame[::-1]\n",
    "\n",
    "            output.clear()\n",
    "            cv2_imshow(frame)\n",
    "            time.sleep(0.1)\n",
    "            # output.clear()\n",
    "\n",
    "            # Store the state, action, next_state, reward, done in memory\n",
    "            agent.memory.push(state, action, next_state, reward, done)\n",
    "            \n",
    "            if global_steps > hParam['BUFFER_SIZE']:\n",
    "                if global_steps % hParam['TARGET_UPDATE'] == 0:\n",
    "                    agent.updateTargetNet()\n",
    "\n",
    "                # Update the target network, copying all weights and biases in DQN\n",
    "                if env.game_over():\n",
    "                    print('Episode: {}  Global Step: {}  Episode Total Reward: {:.4f} Loss: {:.4f}'.format(\n",
    "                       i_episode, global_steps, env.total_reward, loss))\n",
    "\n",
    "                    writer.add_scalar('Episode_total_reward', env.total_reward, i_episode)\n",
    "\n",
    "                    if env.total_reward > best:\n",
    "                        agent.save()\n",
    "                        best = env.total_reward            \n",
    "                \n",
    "                loss = agent.updateQnet()\n",
    "                writer.add_scalar('train_loss', loss, global_steps)\n",
    "            \n",
    "            elif global_steps%500 == 0: \n",
    "                print('steps {}/{}'.format(global_steps, hParam['MAX_ITER']))\n",
    "                \n",
    "            # Move to the next state\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "hParam = {\n",
    "    'BATCH_SIZE': 32,\n",
    "    'GAMMA': 0.99,\n",
    "    'TARGET_UPDATE': 10000,\n",
    "    'EPS_START': 0.1,\n",
    "    'EPS_END': 0.0001,\n",
    "    'MAX_ITER': 2000000,\n",
    "    'DISCOUNT_FACTOR': 0.99,\n",
    "    'LR': 1e-6,\n",
    "    'MOMENTUM': 0.9,\n",
    "    'BUFFER_SIZE': 30000\n",
    "}\n",
    "\n",
    "env = Environment(device, display=True)\n",
    "sungjun = Agent(env.action_set, hParam)\n",
    "train(hParam, env, sungjun)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
